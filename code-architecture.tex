\section{Code Design}
In the following sections to come, we will go over the design and rationale of the choices
we made during the development of the code. In several instances, we will describe 
classes and methods that we believe are beneficial for the reader. Moreover, we will
give examples of the code such that the reader can become more
comfortable with the syntax. As a road map, Figure \ref{fig.phd_design} shows a high level view of the code. As it can be seen, the code consists of six core components:
simulation, integration, temporal control, geometric computation, core fluid
computation and optional fluid computation. This categorization, has lead the code
to be flexible enough to allow modifications without heavily disrupting other
portions of the code. Furthermore, each component can be composed of one or more
subcomponents. In the following sections, we will go into detail of each component and subcomponent.
\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{figures/phd_chart.eps}
        \caption{High level view of the the \texttt{phd} code. The code is compromised of six core components
        that have been color coded. Each component is comprised of one or more classes. Each class
        represent a designated computation. Classes that have ``Base'' in their
        name can be easily modified through inheritance.}
        \label{fig.phd_design}
    \end{center}
\end{figure}

\subsection{Overall Class Design Considerations}
Before we go into specific details of the code, we outline some of the over arching
traits that are common throughout the code. Specifically we will describe field registration,
inheritance and data structures.

\subsubsection{Field Registration}
Form our past experience, we have found that one of the challenging problems in maintaining codes
is that of adding new fields. There are many reasons why to add new fields, for example,
an implementation of new physics module (MHD, radiation, chemistry etc.) or adding a passive scalar
to track some portion of the fluid or modification of an existing algorithm that needs an auxiliary
field which has not been stored. In any case, the addition of a new field can be challenging
if the computation has been hard coded or the data structures, for example \texttt{C} structs, do not
allow for the addition of new fields. For this reason, we adopted a data structure
that allows registration of fields (see Section~\ref{sec.carraycontainer}). In this manner
fields can be easily added in the future.

From these considerations we will see a common thread with our classes, most have a method
for registration or internal addition of fields:
\begin{itemize}
	\item \lstinline{register_fields()} add fields to the simulation.
    \item \lstinline{add_fields()} add fields to self by inspecting current simulation fields.
\end{itemize}
For registration, classes usually inspect the particle container (see Section~\ref{sec.carraycontainer})
and create the fields necessary for the simulation. As an example, gravity registers the acceleration fields
in the proper dimension to the particle container. In this manner, we don't have unnecessary fields during 
the simulation (3D fields for 2D runs). Similarly, classes register fields internally by usually
inspecting the particle container. For example, gravity inspects the particle container to internally 
register the correct dimension of the tree moments.

Since any number of fields can be registered, we have also allowed the ability to group
fields by names. This has been extensively used to group collections of fields, for example,
primitive and conservative fields. This has allowed to easily retrieve groups of fields
that can be computed simultaneously.

\subsubsection{Class Dependencies}
Another source for future problems is when a class depends on another class. For example
our gravity tree solver depends on our load balance implementation. If the algorithms
are designed concurrently, then proper consideration is taken for their interaction.
However, because of the development of new algorithms or experimentations, 
classes will eventually need to interact with others that where not originally designed for. In these
situations common options are to re-factor the algorithm or even rewrite the algorithm itself. Both 
methods can become cumbersome as the code develops and matures and changing one portion of the code 
may create a cascade of unwanted changes.

In our case we decided to write our classes as encapsulated computations (see Figure 
\ref{fig.phd_design}). This means we have
tried to segregate our computations as independent tasks with interactions
through an Application Programming Interface (API). For the most part, each computation
has a base class (i.e \lstinline{ReconstructionBase}, \lstinline{RiemannBase}, 
\lstinline{InetgrateBase} etc.) that
defines the methods it can perform. Moreover, the base class only serves as a template and
returns a warning if accessed. Thus, each implementation inherits the base class and must define
the actual computation. Further, if a method depends another class it has to be written in
such a way that it confines to the API. Therefore, our algorithms are designed to conform to an API
and any future development will have to abide by it.

\subsection{Serialization of classes and parameters}
For the onset, our classes only take \texttt{Python} core data types as arguments for initialization.
This decision was motivated by our restarting and data storing scheme. Each class can be save as
a string representation. Meaning that the name of the class and and all key word arguments are
saved as key string dictionary. This information is enough to query create the class with its
values at any time. Therefor during an output, each class in the simulation is generated its
string information. Then all strings are append into a master list and a json file is created
with the corresponding output. The output has a name reference to the json file. This is the information
needed to restart a file.

\subsection{Particle Data Structures}
\subsubsection{Carray Class}
In choosing the underlying data structure several considerations where taken into account. First, the
data has to be accessible in \texttt{Python} and \texttt{C}. Second, the data structure has to
accommodate several different data types. From these considerations, we wanted a data structure
that closely resembles a \texttt{Numpy} array. \texttt{Numpy} arrays allocate raw data in \texttt{C} and 
allow the user to manipulate it in \texttt{Python} or \texttt{C}. With this design in mind, a
decision had to be made in the underlying structure of the raw data. Two choices where considered, 
either the data would be held in \texttt{C} structs or arrays. The benefit of using structs is that it
can encapsulate all the particle data. Thus, functions could operate on particle by particle basis.
Structs are also easily suited for passing and receiving data from other processors
in parallel runs. Moreover, \texttt{Numpy} has an interface that treats
arrays of structs as a record array. However, this form was abandoned early on
as the attributes of the struct would have to be hard coded and therefore not allow the creation of
dynamic fields at run time. With this consideration, the raw data was chosen to be
\texttt{C} arrays. 

The exact implementation heavily depends on the \lstinline{CarrayBase} class from the \texttt{pysph} code 
\citep{Ramachandran2016}. We have used this class as a starting point and have built functionality around
it. This class can be initialized in \texttt{Python} or \texttt{Cython} and the
interface closely resembles the \texttt{vector} template in \texttt{C++}. Like a \texttt{vector},
a \lstinline{CarrayBase} allows for indexing and dynamic memory management. 
Further, a \lstinline{CarrayBase} can return a \texttt{Numpy} array, allowing the user to use all 
the \texttt{Numpy} functionality (i.e. slicing and fancy indexing). A \lstinline{CarrayBase} comes 
in four different data types, \lstinline{DoubleArray}, \lstinline{IntArray}, \lstinline{LongArray},
and \lstinline{LongLongArray} with \lstinline{CarrayBase} as the parent class for subtype polymorphism.
Below is a simple example using a \lstinline{DoubleArray}.
\begin{lstlisting}
import phd

# allocate carray of size 10 doubles
# and assign values
x = phd.DoubleArray(10)
for i range(len(x)):
	x[i] = i**2

x.append(3.21) # append value at the end of carray
x.resize(5)    # resize to 5 doubles

xnp = x.get_npy_array()      # numpy array reference
xnp[:] = np.arange(x.length) # numpy slice
\end{lstlisting}
In this example a \lstinline{DoubleArray} is created with 10 doubles, then assigned values by indexing. 
Notice that the length of the \lstinline{DoubleArray} can be found by using the \lstinline{len()}
function or the \lstinline{length} attribute. The \lstinline{DoubleArray} then has a value appended to
it followed by a resizing of length of 5. Finally, the \lstinline{get_npy_array()} method is invoked 
returning a \texttt{Numpy} array to be used for slicing. 

\subsubsection{CarrayContainer Class}
\label{sec.carraycontainer}
The use of the \lstinline{CarrayBase} class allows to easily manipulate contiguous data of a certain
type. However, there are many circumstances that an algorithm needs to operate on several 
\lstinline{CarrayBase}s collectively. For example, the Riemann solver needs all the primitive fields
to estimate the fluxes. Although, there is nothing wrong with having a function working on
several \lstinline{CarrayBase}s. However, it can become cumbersome to keep track of all
\lstinline{CarrayBase}s.
Therefore, another data structure has been implemented that stores a collection
of \lstinline{CarrayBase}s of the same size. The data structure is called a \lstinline{CarrayContainer}
and like the \lstinline{CarrayBase} it has many methods to manipulate the underlying data.
Below is listed some of the most common used methods:
\begin{itemize}
	\item \lstinline{register_carray()}: register new \lstinline{CarrayBase} to the container.
	\item \lstinline{get_carray_size()}: size of the \lstinline{CarrayBase}s contained.
    \item \lstinline{remove_items()}: remove elements at given indices from all \lstinline{CarrayBase}s.
    \item \lstinline{resize()}: resize each \lstinline{CarrayBase} to given size.
    \item \lstinline{extract_items()}: return \lstinline{CarrayContainer} with values from given indices.
    \item \lstinline{append_container()}: append \lstinline{CarrayConatiner} to self.
\end{itemize}
As it can be seen from the list, most of the methods operate on the collection of \lstinline{CarrayBase}s
as a whole. The class only allows \lstinline{CarrayBase}s of the same size and produces a runtime error 
otherwise. Additionally, each registered \lstinline{CarrayBase} is associated with a string key such
that it can be retrieved for later use. Below is an example of using the \lstinline{CarrayContainer}.
\begin{lstlisting}
import phd
import numpy as np

# create a container of 10 2d positions
carrys = {"x": "double", "y": "double"}
positions = phd.CarrayContainer(10, carrays)

# assign random values to each carray
size = positions.get_carray_size()
positions["x"][:] = np.random.rand(size)
positions["y"][:] = np.random.rand(size)

# add the z dimension
positions.register_carray(size, "z")
positions["z"][:] = np.random.rand(size)

# create 5 new random positions
carrays = {"x": "double", "y": "double", "z": "double"}
positions2 = phd.CarrayContainer(5, carrays)
size = ca_con.get_carray_size()
for ax in "xyz":
	positions2[ax][:] = np.random.rand(size)

# append new positions to old positions
positions.append_container(positions2)

# remove positions at selected indices
positions.remove(np.array([1, 3, 9])
assert(positions.get_carray_size() == 12)
\end{lstlisting}
In the above example, a \lstinline{CarrayContainer} is created with two \lstinline{DoubleArray}s
labeled ``x'' and ``y'' of size 10 and stored in the \lstinline{position} variable. Then each 
\lstinline{DoubleArray} is retrieved by their key value and assigned random values. Next a new 
\lstinline{DoubleArray} labeled ``z'' is registered to the container and assigned random values.
Then a second \lstinline{CarrayContainer} is created with random values and is append to
\lstinline{position}. Finally, vales at indices 1, 3 and 9 are removed from each \lstinline{DoubleArray}.

% Simulation Class
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Simulation Class}
The \lstinline{Simulation} class is the main driver for advancing the solution in time
and coordinating outputs to disk and terminal. The two most important methods of this
class are listed below:
\begin{itemize}
    \item \lstinline{solve()}: advance \lstinline{IntegrateBase} to its final state while
        outputting all necessary information. 
    \item \lstinline{compute_time_step()}: aggregate all time steps and enforce the smallest.
\end{itemize}
From its inception, the class was designed to be independent of the solvers.
This was accomplished by viewing the simulation as a series of time
advancements. Specifically, \lstinline{IntegrateBase} can only perform a single time step
from its given state (see Section~\ref{sec.integrator} for details) while \lstinline{Simulation}
can dictate when and the number of time steps. Thus, \lstinline{Simulation} controls the
time advancement independently of the equations being solved. As of writing three integrators
exist, however, adding a new integrator is relatively straightforward.

During the course of a simulation the class is responsible to schedule outputs and
determine if the simulation has completed. Simulation outputs and termination are 
designated by the \lstinline{SimulationOutputerBase} and \lstinline{SimulationFinisherBase} 
classes respectively (see Section~\ref{sec.outputters} for details). At the end of every time
step the class calls \lstinline{compute_time_step()} to modify the current time step and
output any necessary data by cycling through all outputers. Likewise, at the beginning of
the time step the class cycles through all \lstinline{SimulationFinisherBase} classes 
in search for a termination signal.

Lastly, the simulation class also controls logging information (see Section~\ref{sec.logging}
for details). Log information is currently printed to the terminal and saved to a log file.
Additionally, the class allows the ability to chose the level of detail for logging.
In parallel runs, the root processor takes responsibility for writing to the log file
and displaying to the terminal.


% Temporal Control and I/O
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Temporal Control and I/O}
In the following sections we will go over the implementations of outputting data 
(\lstinline{ReadWriterBase}), requesting an output (\lstinline{SimulationOutputBase}), and completion
of the simulation (\lstinline{SimulationFinisherBase}). All these methods are controlled by a
\lstinline{SimulationTimeMangager} class which consists of any number of 
\lstinline{SimulationOutputerBase}s and \lstinline{SimulationFinisherBase}s.




\subsubsection{Outputer and Finisher Class}
\label{sec.outputters}

The code has been designed to handle arbitrary signals for outputting data
and completion of the simulation. This has been achieved by introducing the
\lstinline{SimulationOutputerBase} class for output and
\lstinline{SimulationFinisherBase} class for completion. At the end of every
time step (\lstinline{SimulationOutputerBase} includes the beginning and ending
of the simulation) the \lstinline{SimulationTimeManager} calls all registered
\lstinline{SimulationOutputerBase}s and \lstinline{SimulationFinisherBase}s.
During each call the class receives complete control of the integrator. Thus,
the class can inspect the time, time step, iteration, particle data, flux data,
reconstruction data etc. In this way, any viable condition can be applied
for signaling an output or the completion of a simulation. Such an implementation
forgoes the hard coded approach of multiple \texttt{if} \texttt{else} statements
and allows the flexibility to implement as many conditions as needed.

The creation of each \lstinline{SimulationOutputerBase} and \lstinline{SimulationFinisherBase}
must be registered to \lstinline{SimulationTimeManager}. The \lstinline{SimulationTimeManager}
can hold any number of registered classes and it is responsible for maintaining consistency 
across them. For example, both \lstinline{SimulationOutpuerBase} and 
\lstinline{SimulationFinisherBase} are able to modify the time step. Therefore, it is
the responsibility of the \lstinline{SimulationTimeManager} to aggregate all time steps
and enforce the smallest one such that no condition is overlooked.

To conclude this section we give an example, some what trivial but highlights the main points, of
how to create an outputter and finisher. For our example, we use the sedov problem from 
section cite. We are interested to output all time steps once the shock has reached
a density $\rho=2$ and remains less than $\rho=3$. Of course in this situation one may
use the analytical solution to extrapolate when such values would occur. However that
assumes the shock track exactly. In our scenario we let the simulation do the work
for us.

First we begin by defining an outputter with the objective to start outputing
all data once the density value of $\rho=2$ has been reached (see code excerpt). This
is accomplished by inheriting \lstinline{SimulationOutputerBase} and modifying
\lstinline{check_for_output} and \lstinline{modify_timestep}. For over writing
\lstinline{check_for_output}, we check is the the outputter is the main loop state
and if the max density is greater than \lstinline{density_min}. Moreover, there
is no need to change the time step, therefore, it is left unaltered.


\begin{lstlisting}
...
class OutputSedovDensity(SimulationOutputerBase):
    def __init__(self, density_min, base_name="density_output",
                 counter=0, pad=4):
        super(OutputSedovDensity, self).__init__(base_name, counter, pad)
        self.density_min = density_min 

    def check_for_output(self, simulation):
        """Return True to signal the simulation has reached
        sedov interval to ouput data."""
        integrator = simulation.integrator
        state = simulation._state == SimulationTAGS.MAIN_LOOP
        output_sedov = integrator.particles["density"].max() >= self.density_min:

        if state and output_sedov:
            return True
        return False

    def modify_timestep(self, simulation):
        """Return consistent time step."""
        # not modifying
        return simulation.integrator.dt

\end{lstlisting}
At this point our data will be outputted until the simulation is finished. To
create a finisher at the moment when density has reached $\rho_{\mathrm{max}}$
we create a new class \lstinline{SedovDensityFinisher}. This class inherits
\lstinline{SimulationFinisherBase} and one needs to define the \lstinline{finished}
method. In our case we need only to compare our max density values with
$\rho_{\mathrm{max}}$.
\begin{lstlisting}
...
class SedovDensityFinisher(SimulationFinisherBase):
    def __init__(self, density_max, **kwargs):
        self.density_max = density_max

    def finished(self, simulation):
        """Return True to signal the simulation is finished
        if reached max iteration number.
        """
        if simulation.integrator.particles["density"].max() >= self.density_max:
            return True
        else:
            return False

\end{lstlisting}

Although are example was simplistic, we hope that it shows the flexibility and power
what the outputters and finishers may achieve.  

\subsubsection{Reader/Writer Class}
To allow the code to output data in a variety of different formats, we have implemented 
a \lstinline{ReaderWriterBase} class. This allows the user the freedom to
output the data to any desired format. Likewise, this also allows the data to be
read in any format. Every instance of \lstinline{ReaderWriterBase} is associated with a 
\lstinline{SimulationOuputerBase} (see Section~\ref{sec.outputters}) which signals when
output should be created. In this way, multiple outputs can be created at various
moments during the simulation. The standard API of the \lstinline{ReaderWriterBase} is
listed below:
\begin{itemize}
	\item \lstinline{read()} read data in a specific format.
    \item \lstinline{write()} write data in a specific format.
\end{itemize}
With such a framework, new formats can be easily implemented. Moreover, this
allows a simple way to create a front end to read data from other codes. As of writing,
we have implemented a \lstinline{Hdf5} class to read and write particle data in
\texttt{hdf5}\footnote{\url{https://www.hdfgroup.org/solutions/hdf5/}} format. We are
currently in the process of adding new data formats.

Although this framework facilitates the option to save data in different formats,
we believe this framework has far more reaching capabilities. Since the \lstinline{ReaderWriterBase}
has accessed to the integrator, any component of the simulation can be considered for
output. Also, output does not necessarily have to be particle data saved to disk. This framework
can easily be used to create plots, notify that an output has been created through an
email, or even transfer the output to a remote host.

As a simple example, we show below how one can easily track the energy of the system
by using this framework.
\begin{lstlisting}
...
class EnergyTracker(ReaderWriterBase):
	def write(self, base_name, output_directory, integrator):
   		particles = integrator.particles
        
   		# compute square velocity
        v2 = particles["velocity-x"]**2 + particles["velocity-y"]**2
        if len(particles.carray_named_groups["position"]) == 3:
        	v2 += particles["velocity-z"]**2
            
        # calculate kinetic and potential energy of system    
        kinetic = (0.4*particles["mass"]*v2).sum()
        potential = 0.5*(particles["mass"]*particles["potential"]).sum()
        
        message = "Energy values: Kinetic %.2E Gravity %.2E" % (kinetic, potential)
        phdLogger.info(message)
\end{lstlisting}
Our system of interest is a fluid simulation which includes gravity.
First, a new class called \lstinline{EnergyTracker} is created by inheriting \lstinline{ReaderWriterBase}
with the \lstinline{write} method overwritten. The new method simply calculates the total kinetic
and potential energy of the system and then outputs the information to the logger. Then this
class is registered to the appropriate \lstinline{SimulationOutputerBase} and the calculation
will be performed at the appropriate times.

\subsubsection{Logging}
\label{sec.logging}
Event logging during a simulation is performed through the python 
\texttt{logging}\footnote{\url{https://docs.python.org/2/library/logging.html}} library. The library has 
many capabilities but for our intentions we have focused on displaying and storing messages related to the
state of the simulation. To that end, we use one logger called \lstinline{phdLogger}. This 
logger can be imported to any file and used to log any information of interest.

For our purposes, the logger has four levels of logging. They are listed below:
\begin{itemize}
	\item debug: detailed information or diagnosing.
    \item info: working as expected.
    \item success: a successful completion. 
    \item warning: unexpected result that may lead to future problem.
\end{itemize}
The log levels have been listed in order of inclusion. Meaning if ``warning'' is chosen, then all messages
types are logged. However if ``info'' is chosen only messages of type ``debug" and ``info'' are logged.
At runtime the log level and file to store the messages can be specified through the 
\lstinline{Simulation} class.

As an example we show a simple usage of the logger below:
\begin{lstlisting}
import logging
phdLogger = logging.getLogger("phd")
...
phdLogger.info("Starting kinetic energy calculation")
kinetic_energy = particles["mass"]*(particles["velocity-x"]**2 + particles["velocity-y"]**2)

if kinetic_energy.sum() > 0:
	phdLogger.debug("Kinetic energy greater than zero")
\end{lstlisting}
In this example we import the logger and print out a messages related to the calculation of the
kinetic energy. It is important to note that this example is true if run in serial or parallel. The logger
has been modified such that logging information and storage is always handled by the root processor.

\subsection{Integrator Class}
\label{sec.integrator}
To advance the the state of a fluid to a specific time, a suitable integration
scheme must be provided. Although we have laid out the details of the MUSCL-Hancock scheme
in section cite, in practice, many schemes are available with different degrees of 
strengths and weaknesses. Therefore, we found it necessary to have an implementation
that allows the ability to easily switch from integration schemes. To this end, we have
created the \lstinline{IntegrateBase} class which is responsible to advance the state
of the system by one time step. In this way, a simulation is viewed as a series of calls
to \lstinline{IntegrateBase} by the \lstinline{Simulation} class.
Note we say system, not the Euler equations. This
distinction is made because the \lstinline{IntegrateBase} is not limited to the Euler
equations and, in principle, can be used to implement other equations. As an example,
we have implemented a gravitational N-body solver in this framework. We find
this framework to be versatile, allowing the user to chose from different schemes
and allowing the ability to quickly create new schemes for experimentation.
Currently we have implemented a static and moving mesh MUSCL-Hancock and a leap
frog integrator.

To implement an integrator, the \lstinline{IntegrateBase} must be inherited. Below
is the \lstinline{IntegrateBase} api:
\begin{itemize}
    \item \lstinline{before_loop()} perform any needed initialization or initial computations. 
    \item \lstinline{compute_time_step()} compute current state time step.
    \item \lstinline{evolve_time_step()} evolve the state of the system by one time step.
    \item \lstinline{after_loop()} perform any clean up or needed final computations.
\end{itemize}
For each integrator implementation one must define each method from the api. The integrator
has references to all the computation classes as well as state attributes (iteration
counter, time step, and time). The most
involved method is the \lstinline{evolve_time_step()} which defines the equations
and method to be used. For clarity, we shown two integrator examples below. The first
is an implementation of a non moving mesh MUSCL-Hancock integrator.
\begin{lstlisting}
def evolve_timestep(self):
    """Solve the compressible gas equations."""

    phdLogger.info("StaticMeshMUSCLHancock: Starting integration")

    # build left/right states at each face in the mesh
    self.reconstruction.compute_gradients(self.particles, self.mesh,
            self.domain_manager)
    self.reconstruction.compute_states(self.particles, self.mesh,
            self.equation_state.get_gamma(), self.domain_manager, 0.5*self.dt,
            self.riemann.boost)
    self.compute_source("primitive")

    # solve riemann problem, generate flux
    self.riemann.compute_fluxes(self.particles, self.mesh, self.reconstruction,
            self.equation_state)

    # update conservative from fluxes
    self.mesh.update_from_fluxes(self.particles, self.riemann, self.dt)
    self.compute_source("flux")
    self.compute_source("compute")

    self.compute_source("conservative")
    self.domain_manager.update_ghost_fields(self.particles,
            self.particles.carray_named_groups["conservative"],
            True)

    # convert updated conservative to primitive
    self.equation_state.primitive_from_conservative(self.particles)
    self.iteration += 1; self.time += self.dt
\end{lstlisting}
As it can be seen the method is a series of core computations. The method
begins by calculating the gradients followed by computing the left/right
states for the riemann solver. Once the fluxes are calculated they are used
to update the fields. Sprinkled in are calls to source terms, if any.
Finally the ghost particles, iteration counter and
time are updated and the system is ready for the next computation.


% Geometric Computation
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Geometric Computation}
\subsubsection{Mesh Class}
The mesh class \lstinline{Mesh} is responsible for all geometric computations relating
to the particle cell. For example the class computes the cells volume and center of mass.
The class works in tandem with the \lstinline{DomainManager} to build the mesh.
Once constructed the mesh class register geometric fields that will be needed during
the simulation.

The tessellation algorithm section is performed through \lstinline{build_geometry()}.
It works withe the \lstinline{DomainManger} building the mesh in rounds. In the first
round all local particles are added to the tessellation, at this point there are no
ghost particles. Particles that have infinite volume or that have radius that intersect
with other processor boundaries, in parallel, or the simulation domain are flagged. Then the 
\lstinline{DomainManager} is called creating the necessary ghost particles. The mesh is
updated with the ghost particles and process is repeated. This process continues until
all particles have a finite volume and all neighbors have been accounted for.

With the tessellation complete, cell values are computed. This entail calculating
cell neighbors, volumes, center of masses, face area vectors etc. The information
regrading to the faces of each cell is stored an attribute \lstinline{faces}. In this
data structure, each column is a face. For each face entry there exists references to
the two particles that define the face. For each particle the neighbor information
is reference through faces. In this way, when a particle wants all of its neighbors
it first request all the faces relating to its cells. Then for each face, the neighboring
particle can be retrieved.

Once the flux have calculated, it is the responsibility of the mesh to use the fluxes
to update the values of the cells. This was chosen as the equation of update involves
geometric terms. Further this allows a framework to allow different structured meshes
(spherical and cylindrical). Since all geometric quantities are regulated to the mesh

\subsubsection{Domain Manager}
Our \lstinline{DomainManager} class is designed to handle information associated with
the domains of the simulation. We consider a domain to be the spatial region where the computation
is performed. In serial, this is the entire spatial region of the simulation.
In parallel, this is the spatial region associated with each processor. 
In this way, for the most part, our domains are isolated computations and when
data is needed from neighboring domains they are requested through the \lstinline{DomainManager}.

The \lstinline{DomainManager} was initially designed to contain the limits and dimension of
the problem. As development continued it became natural that the \lstinline{DomainManager} would
encompass the boundary condition, ghost particle information, and exchange of data across boundaries.
At its current state, the \lstinline{DomainManager} only supports communication of ghost particle data
and does not support the general forms of data communication
(i.e reduction, gathers, broadcast etc). We plan in the next revision to implement an api that
performs these task. In doing so, we would remove strict dependence on MPI functions and only
work through the \lstinline{DomainManager}.

%\subsubsection{Internal Boundary Particle Sharing}
In parallel, particles are decomposed into a disjoint set of spatial domains
with each domain mapped to a unique processor. The construction
of the global Voronoi mesh is then delegated to the construction of a disjoint set
of local Voronoi meshes. For the set of local constructions to be consistent
with the global mesh, the appropriate boundary particles must be 
communicated across domains.

Interior ghost particles, are particles formed from neighboring domains and are used to
connect local meshes together. Their creation is handled by the \lstinline{DomainManager}
through the \lstinline{create_interior_ghost_particles()} method. This method
creates interior ghost particles by inspecting the search radius of
each local particle. If a particles search radius overlaps with a processor
boundary then it is flagged and a corresponding ghost particle is created and exported to that processor.
The search method is made possible by the \lstinline{DomainManager}s ability to query
all domains through geometrical searches. Care is taken such that no duplicates are created through the 
whole process. After each particle is inspected, ghost particles are communicated and then the
mesh is allowed to updated itself. This procedure is repeated until particles are no longer flagged.

%\subsubsection{Particle Motion}
After a flux update, the particles are allowed to move. Depending on initial particle position and
velocity a particle may leave its processor or the simulation domain. In either case, the 
\lstinline{DomainManager} is responsible for the destintation of the particle.

%\subsubsection{Data Communication}
After the creation of ghost particles, in parallel simulations, supplemental data has to be communicated. 
For example, the center of mass may not be computed locally because its neighbors may not exist locally.
This is because our implementation of ghost particles only to guarantees all neighbors of local
particles. Certainly we could of opted to import all neighbors of ghost particles but instead we have 
decided to communicate that information instead. Thus, the \lstinline{DomianManager} records all information
associated with a ghost particle. This allows ghost particles to be easily updated with any data. These
operations are implemented through \lstinline{update_ghost_fields()} and 
\lstinline{update_ghost_gradients()}


When a particle leaves the simulation domain in serial or parallel the \lstinline{DomainManager}
flags that particle and then applies the boundary condition. For reflective boundaries the particle is not
allowed to leave and for periodic boundaries the particle is wrapped back into the domain. If the simulation
is in parallel the \lstinline{DomainManager} exports the particle to the correct processor and removes
it from the local particle container. If a particle leaves its processor patch the \lstinline{DomainManager}
will query all domains and find the correct domain for export.

\subsubsection{Boundary Condition}
Similar to interior ghost particles, exterior ghost particles are used to
complete local meshes. However, exterior ghost particles are not from neighboring
domains but instead are created through a boundary condition. To allow for different
boundary condition we have created \lstinline{BoundaryConditionBase} that interacts
with \lstinline{DomainManager}. The api is listed below:
\begin{itemize}
	\item \lstinline{create_ghost_particle()} create ghost particle from flagged particle.
    \item \lstinline{migrate_particles()} for particles that have left the domain, apply
    appropriate boundary condition.
    \item \lstinline{update_gradients()} apply boundary condition to ghost particle gradients.
    \item \lstinline{update_fields()} apply boundary condition to ghost particle fields.
\end{itemize}
Through this api, any boundary condition may be implemented. The boundary condition does not have
to be uniform in each dimension. Mixed boundaries or even problem specific boundaries are allowed
in this framework. Furthermore, the api allows the boundary
condition to modify particle motion and field data. In this way, we have extracted all boundary
information from the \lstinline{DomainManager}.

Currently, we have two implementations of boundary conditions, reflective and periodic.
In the reflective case, the flagged particle is mirrored across each boundary edge of the
simulation. If an intersection occurs, a ghost particle is created with its normal velocity flipped.
In the serial case, the ghost particle is added directly to the particle data container. In parallel, the 
ghost particle is then further inspected for intersection of neighboring processors. The ghost particle
is then placed in a communication to be exported to each flagged processor. For the periodic case
the procedure is similar, with the exception that particle is periodically shifted
instead of being mirrored.

\subsubsection{Load Balance}
Our load balance scheme described in Section is implemented through the \lstinline{LoadBalance}
class. The method \lstinline{distribute()} performs the redistribution of particles across processors in a 
parallel run. The \lstinline{LoadBalance} class is essentially the API. Internally, it has a reference
to a \lstinline{Tree} class that creates a tree from the Hilbert keys and performs all the necessary
operations. This class was one of the first class created before the main ideas of the code worn born
out. Therefore, this class will be heavily modified in the next revision. First, the API needs to be
become independent of the internal implementation. Second, we want to generalize the load balance
scheme to allow more mature load balancing scheme. Our current load balance scheme is a simplistic
and more optimized implementations exists. For example we would like to follow pysph and use the Zoltan 
\citep{Devine2000}framework. Zoltan is an mature framework offering geometric,
graph-based, hypergraph-based partioning algorithms.


% Core Fluid Computation
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Core Fluid Computation}
\subsubsection{Riemann Solver}
The fluxes at each face of the mesh is constructed by the \lstinline{RiemannBase} class.
As a common thread with all of our computation classes, the base class defines the api.
The api is straight forward with only two methods to define:
\begin{itemize}
	\item \lstinline{riemann_solver()} solve for fluxes at each interface.
    \item \lstinline{compute_time_step()} compute hydro time step.
\end{itemize}
The \lstinline{compute_time_step()} is optional since the base class has a default
CFL constrained time step calculation. However the option is there if a particular 
solver implementation needs to modify it. Furthermore, the base class defines a 
method for the fluxes to be transformed back into the lab frame. This allows,
for the most part, to implement a solver without worrying about the details of
rotating and boosting back into the lab frame. As of time of writing, we have
implemented the HLL, HLLC, and Exact Riemann solvers.

\subsubsection{Reconstruction}
Once the mesh is created, cell center values need to be reconstructed to cell center faces
in order to calculate the fluxes. The interface to implement a reconstruction
scheme is dictated by the \lstinline{ReconstructionBase} class which has the following api:
\begin{itemize}
	\item \lstinline{compute_gradients()} calculate the gradients of the fields for each particle.
    \item \lstinline{add_spatial()} add gradients to expansion.
    \item \lstinline{add_temporal()} add time derivatives to expansion.
\end{itemize}
The api embodies a general deconstruction of eq. into three building blocks. First, is the
calculation of the spatial derivatives (i.e gradients) by \lstinline{compute_gradients()}.
In this call the derivatives are computed through a specified scheme. For example, the gradients can be 
calculated by the least square approach \citep{Pakmor2016} or through geometrical properties 
\citep{Springel2010}. With the derivatives calculated cell center values can then be extrapolated spatially
(\lstinline{add_spatial()}) or temporally (\lstinline{add_temporal()}). In this manner the
user has complete control of the terms in the expansion.This is important to note, as many integrator 
schemes make use of the reconstruction several times in Runge-Kutta solver variants (i.e \cite{Pakmor2016} 
and \cite{Duffell2011}).

\subsection{Equation of State}
Due to the implementation of the reconstruction and Riemann solvers, primitive fields
must be computed. The computation of primitive fields requires an equation of state. 
Since there are multiple formulations of an equation of state we have 
decided to implement an API through the \lstinline{EquationStateBase} class. The methods
are listed below:
\begin{itemize}
    \item \lstinline{conservative_from_primitive()}: compute conservative fields from primitive.
    \item \lstinline{primitive_from_conservative()}: compute primitive fields form conservative.
    \item \lstinline{sound_speed()}: compute the sound speed of the fluid.
    \item \lstinline{get_gamma()}: compute the ratio of specific heats.
\end{itemize}
The \lstinline{EquationStateBase} is responsible for converting the fields from conservative
to primitive and vice versa. Furthermore, the equation of state can calculate the sound
speed of the fluid. We have found this formulation to adequately remove the details of the
state of the fluid from other calculations.

Currently the \lstinline{EquationStateBase} is simplistic. However, in the next revision we plan
to extend its functionality to include the computation of chemistry species. This is because a chemistry 
solver is being developed.

% Optional Fluid Computation
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Optional Fluid Computation}
\subsubsection{General Source Terms}
The inclusion of source terms has been implemented through a general source
\lstinline{SourceTermBase} with methods listed below:
\begin{itemize}
    \item \lstinline{apply_primitve()}: modify primitive variables.
    \item \lstinline{apply_conservative()}: modify conservative variables.
    \item \lstinline{apply_flux()}: modify flux terms.
    \item \lstinline{compute_source()}: calculate source components.
    \item \lstinline{compute_time_step()}: calculate time step from source term.
\end{itemize}
This formulation was chosen after some considerable experimentation. One of the
earlier attempts was a registration process. In this scenario a source term
would link any computation to class method. This information was stored
in a dictionary inside \lstinline{Simulation} and at runtime each class method
was over written using Pythons decorator scheme. Although, this scheme makes
use of more advance programming methods we found that this implementation was
not easy to discern. Moreover we found that this method left to much ambiguity
to the source terms, allowing . Instead we
found this current implementation to be more understandable and easier to
generalize.

The api listed above are the mandatory methods to be defined for any source
term. In general, we write an interface for each given source term. In this
way if the integrator changes only the api has to be updated leaving
the core of the source term unaltered. This also allows to easily implement
third party libraries as source terms, chemistry or radiation for example.
Once the methods have been defined the class is registered to 
\lstinline{Simulation} class and will be invoked at the appropriate parts
during the computation.

\subsubsection{Gravity}
For a more concrete example of how to include a source term we highlight the
pieces used to include self gravity. Our self gravity is a tree based implementation
named \lstinline{GravityTree}. Its main routine is \lstinline{walk()} which is the
calculation of gravitational accelerations from the current position of the particles.
To include this as a source term we created a new class called \lstinline{SelfGravity}
which inherits \lstinline{SourceTermBase}. As discussed in section, gravity alters
the momentum and primitive values.
\begin{lstlisting}
...
cdef class SelfGravity(MUSCLHancockSourceTerm):
    ...
    cpdef apply_primitive(self, object integrator):
        # loop over each face in the mesh 
        for m in range(integrator.mesh.faces.get_carray_size()):
            ....
            # add gravity to velocity
            for k in range(dim):
                vl[k][m] += 0.5*dt*a[k][i]
                vr[k][m] += 0.5*dt*a[k][j]

        # add gravity acceleration from particle
        for i in range(integrator.particles.get_carray_size()):
            ...
                for k in range(dim):
                    # update momentum
                    e.data[i] += 0.5*dt*mv[k][i]*a[k][i]

                    # update energy 
                    mv[k][i] += 0.5*dt*mass.data[i]*a[k][i]

    cpdef apply_conservative(self, object integrator):
        ...
        # add gravity acceleration from particle
        for i in range(integrator.particles.get_carray_size()):
            ...
                for k in range(dim):
                    # update momentum
                    mv[k][i] += 0.5*dt*mass.data[i]*a[k][i]

                    # update energy 
                    e.data[i] += 0.5*dt*mv[k][i]*a[k][i]
\end{lstlisting}
